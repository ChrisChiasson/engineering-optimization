<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="Unconstrained_Nonlinear_Optimization"
	xmlns="http://docbook.org/ns/docbook"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:xl="http://www.w3.org/1999/xlink"
	xmlns:xml="http://www.w3.org/XML/1998/namespace"
	version="5">
	<title>Unconstrained Nonlinear Optimization  &#x2014; Homework 2</title>
	<titleabbrev>Unconstrained Nonlinear Optimization</titleabbrev>
	<para>As Dr. Mohamed El-Sayed shows in
		<xref linkend="hw_2_screenshot_assignment"/>, the assignment is to solve
		<olink targetdoc="self" targetptr="GNVNOTED"/> problems 3-2 through 3-7.
		The block quotation below gives the problem statements in their
		entireties. By the way, I realize screenshot says <quote>Ex:,</quote>
		but since there are no more examples in chapter 3 after example 3-3 and
		Dr. El-Sayed verbally mentioned problems rather than examples, I am
		doing the problems.</para>
	<figure xml:id="hw_2_screenshot_assignment">
		<title>Homework 2 Assignment</title>
		<screenshot>
			<mediaobject>
				<imageobject role="html">
					<imagedata fileref="mout/hw_2_screenshot_assignment.png"/>
				</imageobject>
				<imageobject role="fo">
					<imagedata contentwidth="5in" 
						fileref="mout/hw_2_screenshot_assignment.png"/>
				</imageobject>
				<textobject>
					<phrase>Ex: (3-2)&#x2014;(3-7) HW. underlined followed by
						Chapter (4) underlined</phrase>
				</textobject>
			</mediaobject>
		</screenshot>
		<caption>
			<para>This is a screenshot from part B of the fourth lecture at
				8:36.</para>
		</caption>
	</figure>
	<blockquote>
		<attribution><citation><olink targetdoc="self" targetptr="GNVNOTED"/></citation>, p129</attribution>
		<literallayout><emphasis role="strong">3-2</emphasis> Given the unconstrained minimization problem
      Minimize: <xi:include href="mout/hw_2_poly.xml"/>
                                          <xi:include href="mout/hw_2_start.xml"/>
      Apply three iterations of the method of steepest descent to this problem.
      List the values of all critical parameters at each step.
<emphasis role="strong">3-3</emphasis> Solve Prob. 3-2 using the Fletcher-Reeves conjugate direction method.
<emphasis role="strong">3-4</emphasis> Solve Prob. 3-2 using the Davidon-Fletcher-Powell variable metric algorithm.
<emphasis role="strong">3-5</emphasis> Solve Prob. 3-2 using the Broyden-Fletcher-Shanno-Goldfarb algorithm.
<emphasis role="strong">3-6</emphasis> Solve Prob. 3-2 using Powell's method. Use six search directions.
<emphasis role="strong">3-7</emphasis> Solve Prob. 3-2 using Newton's method.</literallayout>
	</blockquote>
	<para>The solution to all of the quoted questions is presented in
		<xref linkend="hw_2_minimization_paths"/>,
		<xref linkend="hw_2_ungraphed_critical_parameters_iHess"/>,
		and <xref linkend="hw_2_ungraphed_critical_parameters_nonIHess"/>.
		The	calculations are available in
		<olink targetdoc="self" targetptr="HW2_code"/>. I suppose I could omit any
		further comments, since the referenced materials answer the questions.
		However, I think some comments	are appropriate.</para>
	<para>A first thing to note about the graph is that I have iterated each
		method to converge on the minimum, not just through	the requisite three
		steps. Doing so involved actually programming each method, instead of
		completing the problems <quote>by hand</quote>. In lecture, Dr. El-Sayed
		argued that programming	each method is more a test of programming skill
		than engineering acumen. I somewhat agree, but I used the plot and table
		as a way to debug my work. So my programming of all the methods was
		meant to help me rather than to show off any programming skill.</para>
	<xi:include href="mout/hw_2_minimization_paths.xml"/>
	<para>The graph shows the search direction and step size for each iteration.
		For some methods, the graph does not show all critical information used
		to obtain the search direction. For instance, while contours and thus
		gradients (perpendicular to contours) are easy to infer from the graph,
		the Hessian matrix isn't, at least for me. For this reason, I have
		included <xref linkend="hw_2_ungraphed_critical_parameters_iHess"/> and
		<xref linkend="hw_2_ungraphed_critical_parameters_nonIHess"/>.</para>
	<xi:include href="mout/hw_2_ungraphed_critical_parameters_iHess.xml"/>
	<xi:include href="mout/hw_2_ungraphed_critical_parameters_nonIHess.xml"/>
	<para>Since this problem has a quadratic objective function, each method's
		ability to determine a search directions parallel to the negative of
		<xi:include	href="mout/hw_2_ideal_direction.xml"/>, the ideal
		direction for this problem, determines how well it will do relative to
		the others. The ideal direction is otherwise known as the negative
		inverse of the Hessian dotted with the gradient. Physically, it is a
		vector that points from any location,
		<xi:include href="mout/hw_2_domain.xml"/>, directly to the
		minimum	at
		<xi:include href="mout/hw_2_minimum.xml"/>.
		To help the reader visualize the ideal and negative gradient fields for
		this problem, I have included both field plots superimposed on contours
		and the	optimization paths in
		<olink targetdoc="self" targetptr="hw_2_minimization_paths_negative_inverse_hessian_dot_gradient"/>
		and
		<olink targetdoc="self" targetptr="hw_2_minimization_paths_negative_gradient"/>.</para>
	<section xml:id="hw_2_method_detail">
		<title>Method Discussion</title>
		<section xml:id="hw_2_sd_detail">
			<title>Steepest Descent</title>
			<para>The method of steepest descent is fairly easy to
				understand. It follows a direction that is perpendicular to a
				contour line in the declining direction from its
				location. This direction is the negative of the gradient. As can
				be seen from
				<olink targetdoc="self" targetptr="hw_2_minimization_paths_negative_gradient"/>,
				it zig-zags in perpendicular directions because the minimum of
				the	line search along a gradient is located at the tangent of a
				contour and the line search. The method takes
				<quote>many</quote> iterations relative to the others, but does
				converge.</para>
		</section>
		<section xml:id="hw_2_fr_detail">
			<title>Fletcher-Reeves</title>
			<para>To be honest, I do not understand how the	Fletcher-Reeves
				conjugate direction method works on a theoretical level. It
				makes sense that turning the gradient a	bit via vector addition
				in the direction of the previous gradient will give a more
				controlled descent.
				<olink targetdoc="self" targetptr="PTVFNRC"/> &#167; 10.6
				carries a brief theoretical description. The vector
				addition for the search direction, S, on iteration q is
				<xi:include href="mout/hw_2_fletcher_reeves_direction.xml"/>,
				where <xi:include href="mout/hw_2_fletcher_reeves_beta.xml"/>.
				I don't know if this method qualifies as quasi-Newton, but it
				certainly isn't just first order.</para>
		</section>
		<section xml:id="hw_2_dfp_detail">
			<title>Davidon-Fletcher-Powell (DFP)</title>
			<para>As mentioned in
				<olink targetdoc="self" targetptr="GNVNOTED"/> at the beginning
				of &#167; 3-3.3	Variable Metric Methods, the DFP quasi-Newton
				method builds an N by N<footnote><para>N is the number of
				variables in the optimization problem. In this case, N is
				two.</para></footnote> matrix to approximate the inverse of
				F's Hessian, which should help it converge to the minimum in
				fewer iterations than Fletcher-Reeves, due to the retention of
				more information and use of more computations per step. In my
				limited testing of non-quadratic functions, the Variable
				Metric Methods did converge in fewer iterations than the
				Fletcher-Reeves method.</para>
		</section>
		<section xml:id="hw_2_bfgs_detail">
			<title>Broyden-Fletcher-Shanno-Goldfarb (BFGS)</title>
			<para>As mentioned in
				<olink targetdoc="self" targetptr="MHEM"/>, the BFGS method is
				another <link xl:href="http://eom.springer.de/Q/q120050.htm">quasi-Newton
				method</link> (includes direction update equations) in the
				Broyden family. From that source, I learned that the <link
				linkend="hw_2_minimization_paths">search points generated by
				both the DFP and BFGS methods are identical</link>, which is
				confirmed by my plot. Also, the BFGS method is supposedly more
				tolerant to loss of numerical precision, which I have not tried
				to verify.</para>
		</section>
		<section xml:id="hw_2_pow_detail">
			<title>Powell</title>
			<para>Powell's method is very special because it is a zero order
				method, meaning it doesn't require a symbolic gradient or
				numerical construction thereof. The tradeoff for this is a
				relatively high number of function evaluations. The Powell
				method finds search directions every N steps by discarding 
				previous ones for an average direction over the last N
				steps. Also, after each step, the search direction matrix column
				corresponding to the present step number is replaced with the
				actual step vector.</para>
			<para>The forumlation given in <olink targetdoc="self"
				targetptr="GNVNOTED"/>'s &#167; 3-2.2 only provides for an
				initial search in each of the N independant directions. I am
				not sure why the book mentions six starting directions in the
				assignement.</para>
		</section>
		<section xml:id="hw_2_in_detail">
			<title>Newton</title>
			<para>This method is ideal for this type of problem,
				where the negative of the Hessian inverse dotted with the
				gradient at any point in the domain leads directly to the
				minimum, the definition of the objective is simple, and the
				number of independant variables is low. Because the search
				direction points exactly to the minimum, the method moves to the
				minimum in one step.</para>
		</section>
	</section>		
</chapter>